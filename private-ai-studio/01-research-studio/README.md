# Module 1: Research Studio
**Build Your RAG-Powered Knowledge Synthesis Environment**

*Part of GenCreator Labs by Frank*

---

## ðŸŽ¯ What You'll Learn

- Deploy OpenWebUI as your research command center
- Configure RAG (Retrieval-Augmented Generation) with document ingestion
- Set up knowledge bases with proper chunking and embeddings
- Integrate MCP servers for real-time research tools
- Build citation-backed research workflows
- Connect to local models (Ollama) for privacy-first research

---

## ðŸ“ Module Structure

```
01-research-studio/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ 01-openwebui-deployment/       # OpenWebUI setup
â”œâ”€â”€ 02-rag-configuration/          # RAG engine tuning
â”œâ”€â”€ 03-knowledge-base-setup/       # Document management
â””â”€â”€ 04-mcp-integration/            # MCP server connections
```

---

## ðŸ› ï¸ Deployment Options

### Option A: Minimal (OpenWebUI Only)

```bash
# Single container, basic setup
docker run -d --name open-webui \
  -p 3000:8080 \
  -e WEBUI_SECRET_KEY=$(openssl rand -hex 32) \
  -v open-webui:/app/backend/data \
  ghcr.io/open-webui/open-webui:main
```

**Access**: http://localhost:3000

---

### Option B: Production (Full Stack)

```bash
cd docker
cp .env.example .env
# Configure all settings in .env
docker compose -f docker-compose.research.yml up -d
```

**Services**:
- OpenWebUI (port 3000)
- Qdrant Vector DB (port 6333)
- Ollama (port 11434)
- Traefik reverse proxy (port 80/443)

---

## ðŸ”¬ RAG Configuration

### Understanding the RAG Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG PIPELINE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  USER QUERY                                                     â”‚
â”‚      â”‚                                                         â”‚
â”‚      â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚  Query Rewrite  â”‚  â† Enhance query for retrieval            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                    â”‚
â”‚           â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚  Vector Search  â”‚  â† Find relevant chunks                   â”‚
â”‚  â”‚   (Qdrant)      â”‚     using embeddings                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                    â”‚
â”‚           â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚  Context Window â”‚  â† Top-K chunks merged                   â”‚
â”‚  â”‚    (16K-128K)   â”‚     with conversation context            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                    â”‚
â”‚           â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚   LLM Response  â”‚  â† Generate with retrieved context       â”‚
â”‚  â”‚   (Ollama)      â”‚     + citations                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Optimal RAG Settings for Research

```yaml
# In OpenWebUI Admin Settings â†’ RAG

Retrieval Settings:
  Chunk Size: 2048 tokens
  Chunk Overlap: 256 tokens
  Top-K Results: 10
  Similarity Threshold: 0.7

Context Settings:
  Context Window Size: 32768 tokens
  RAG Percent of Context: 80%
  RAG Query Generator Model: llama3.2:latest

Citation Settings:
  Enable Citations: true
  Citation Style: APA
  Include Source URLs: true
```

### Handling Ollama Context Limits

**Problem**: Ollama defaults to 2048 tokens, severely limiting RAG

**Solution**: Increase context window in Ollama

```bash
# Check current context limit
curl http://localhost:11434/api/version

# For llama3.2 with 128K context
ollama run llama3.2:latest
# In the Ollama session:
/set parameter num_ctx 131072
/set parameter num_predict 4096

# Or set via environment
export OLLAMA_NUM_PARALLEL=1
export OLLAMA_NUM_CTX=131072
```

---

## ðŸ“š Knowledge Base Management

### Document Processing Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DOCUMENT INGESTION                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  RAW DOCUMENTS                                                  â”‚
â”‚  â”œâ”€â”€ PDF (research papers, reports)                            â”‚
â”‚  â”œâ”€â”€ Markdown (notes, docs)                                    â”‚
â”‚  â”œâ”€â”€ HTML (web content)                                       â”‚
â”‚  â”œâ”€â”€ Word documents                                            â”‚
â”‚  â””â”€â”€ Audio (transcribed via Whisper)                          â”‚
â”‚                                                                  â”‚
â”‚       â”‚                                                        â”‚
â”‚       â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  PARSING LAYER                          â”‚   â”‚
â”‚  â”‚  â€¢ PDF â†’ text (pdfminer, pypdf)                        â”‚   â”‚
â”‚  â”‚  â€¢ Markdown â†’ structured text                          â”‚   â”‚
â”‚  â”‚  â€¢ HTML â†’ cleaned text + metadata                      â”‚   â”‚
â”‚  â”‚  â€¢ Audio â†’ Whisper transcription                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                        â”‚
â”‚       â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 CHUNKING LAYER                          â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  Strategy: Semantic Chunking                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ Sentence 1 â”‚ Sentence 2 â”‚ ... â”‚ Sentence N     â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚              â”‚              â”‚           â”‚               â”‚   â”‚
â”‚  â”‚              â–¼              â–¼           â–¼               â”‚   â”‚
â”‚  â”‚         Semantic boundaries detected by:               â”‚   â”‚
â”‚  â”‚         â€¢ Topic shifts (embedding distance)            â”‚   â”‚
â”‚  â”‚         â€¢ Paragraph breaks                            â”‚   â”‚
â”‚  â”‚         â€¢ Section headers                             â”‚   â”‚
â”‚  â”‚         â€¢ Token limits (2048 max)                     â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                        â”‚
â”‚       â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                EMBEDDING LAYER                          â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  Model: nomic-embed-text:latest (8192 dim)             â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ Chunk 1: [0.23, -0.45, 0.89, ..., 0.12]        â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ Chunk 2: [-0.12, 0.67, -0.34, ..., 0.45]        â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ Chunk 3: [0.78, 0.23, -0.56, ..., -0.23]        â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                        â”‚
â”‚       â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              VECTOR STORAGE (Qdrant)                    â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  Collection: research_knowledge                         â”‚   â”‚
â”‚  â”‚  â€¢ Payload: source, title, page, chunk_id              â”‚   â”‚
â”‚  â”‚  â€¢ Index: HNSW for fast similarity search              â”‚   â”‚
â”‚  â”‚  â€¢ Distance: Cosine similarity                         â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Recommended Knowledge Base Structure

```
knowledge-base/
â”œâ”€â”€ research-papers/
â”‚   â”œâ”€â”€ machine-learning/
â”‚   â”‚   â”œâ”€â”€ papers/
â”‚   â”‚   â””â”€â”€ summaries/
â”‚   â”œâ”€â”€ ai-safety/
â”‚   â”‚   â”œâ”€â”€ papers/
â”‚   â”‚   â””â”€â”€ summaries/
â”‚   â””â”€â”€ domains/
â”‚       â”œâ”€â”€ healthcare/
â”‚       â”œâ”€â”€ finance/
â”‚       â””â”€â”€ science/
â”œâ”€â”€ documentation/
â”‚   â”œâ”€â”€ technical/
â”‚   â”‚   â”œâ”€â”€ api-docs/
â”‚   â”‚   â””â”€â”€ architecture/
â”‚   â””â”€â”€ user-guides/
â”‚       â”œâ”€â”€ how-to/
â”‚       â””â”€â”€ tutorials/
â”œâ”€â”€ personal-notes/
â”‚   â”œâ”€â”€ meeting-notes/
â”‚   â”œâ”€â”€ project-docs/
â”‚   â””â”€â”€ journal/
â””â”€â”€ web-archives/
    â”œâ”€â”€ saved-articles/
    â””â”€â”€ curated-resources/
```

---

## ðŸ”— MCP Server Integration

### Native MCP Support (v0.6.31+)

OpenWebUI includes native MCP support. Configure in **Admin â†’ External Tools â†’ MCP Servers**:

```json
{
  "mcpServers": {
    "arxiv": {
      "command": "npx",
      "args": ["-y", "@openwebui/mcp-server-arxiv"],
      "env": {}
    },
    "web-search": {
      "command": "npx",
      "args": ["-y", "@openwebui/mcp-server-web-search"],
      "env": {}
    },
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem"],
      "env": {
        "PATH_TO_ALLOW": "/data/knowledge-base"
      }
    }
  }
}
```

### Recommended Research MCPs

| MCP Server | Purpose | Install Command |
|------------|---------|-----------------|
| arXiv | Search ML/AI papers | `npx -y @openwebui/mcp-server-arxiv` |
| Web Search | Real-time web queries | `npx -y @openwebui/mcp-server-web-search` |
| GitHub | Code search, repo access | `npx -y @modelcontextprotocol/server-github` |
| Wikipedia | Knowledge queries | `npx -y @openwebui/mcp-server-wikipedia` |
| Brave Search | Alternative web search | `npx -y @openwebui/mcp-server-brave-search` |

### Custom MCP Server Example

```typescript
// mcp-servers/research-helper/index.ts
import { Server } from "@modelcontextprotocol/sdk/server/index.js";

const server = new Server({
  name: "research-helper",
  version: "1.0.0",
});

server.setRequestHandler("tools/list", async () => ({
  tools: [
    {
      name: "summarize_paper",
      description: "Summarize a research paper from URL or file",
      inputSchema: {
        type: "object",
        properties: {
          source: { type: "string", description: "URL or file path" },
          length: { type: "string", enum: ["short", "medium", "long"] }
        },
        required: ["source"]
      }
    },
    {
      name: "find_citations",
      description: "Find all citations of a paper",
      inputSchema: {
        type: "object",
        properties: {
          doi: { type: "string", description: "Paper DOI" },
          limit: { type: "number", default: 10 }
        },
        required: ["doi"]
      }
    },
    {
      name: "compare_claims",
      description: "Compare claims across multiple papers",
      inputSchema: {
        type: "object",
        properties: {
          claim: { type: "string", description: "Claim to verify" },
          papers: { type: "array", description: "Paper DOIs to check" }
        },
        required: ["claim"]
      }
    }
  ]
}));

// Implementation details...
export default server;
```

---

## ðŸ“Š Monitoring Research Quality

### Metrics to Track

| Metric | Target | Why |
|--------|--------|-----|
| Citation Accuracy | >95% | Claims backed by sources |
| Response Relevance | >85% | Retrieved context is useful |
| Chunk Utilization | >70% | Retrieved chunks used in response |
| Latency (RAG) | <5s | Research flow maintained |
| Knowledge Base Coverage | Growing | New documents added regularly |

### Setup Monitoring Dashboard

```yaml
# docker/monitoring/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'openwebui'
    static_configs:
      - targets: ['openwebui:3000']
  
  - job_name: 'qdrant'
    static_configs:
      - targets: ['qdrant:6333']
  
  - job_name: 'ollama'
    static_configs:
      - targets: ['ollama:11434']
```

---

## ðŸš€ Quick Start

```bash
# Start Research Studio
cd docker
docker compose -f docker-compose.research.yml up -d

# Access OpenWebUI at http://localhost:3000
# First user becomes admin

# In OpenWebUI:
# 1. Settings â†’ Knowledge â†’ Create knowledge base
# 2. Settings â†’ External Tools â†’ Configure MCP servers
# 3. Upload documents to knowledge base
# 4. Start chatting with RAG enabled
```

---

## ðŸ“š Next Steps

**[â†’ Module 2: RAG Configuration â†’](02-rag-configuration/)**

---

*Part of [GenCreator Labs](https://frankx.ai/gencreator) by Frank*
